{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1wpl-f2KuPp"
      },
      "source": [
        "# Parsing Algorithms\n",
        "\n",
        "![LexPars](https://raw.githubusercontent.com/ValRCS/RTU_Algorithms_DIP321/refs/heads/main/imgs/lexer_parser.jpg)\n",
        "\n",
        "Src: Guide to Parsing Algorithms and Terminology - https://tomassetti.me/guide-parsing-algorithms-terminology/\n",
        "\n",
        "**Overview of Parsing**\n",
        "Parsing is the process of analyzing a raw input (typically source code) according to the rules of a formal grammar to uncover its underlying structure . In compilers and interpreters, parsing bridges the gap between human-readable code and machine-friendly representations by transforming text into structured trees for further semantic checks or code generation .\n",
        "\n",
        "## Why should you care about parsing?\n",
        "\n",
        "Parsing is a fundamental concept in computer science, especially in the context of programming languages and compilers. Understanding parsing helps you:\n",
        "* **Design languages**: Knowing how parsing works allows you to create languages such as Domain Specific Languages with clear and unambiguous syntax.\n",
        "* **Implement compilers**: Parsing is a crucial step in compiling code, and understanding it helps you build efficient and effective compilers.\n",
        "* **Debug and optimize**: Understanding parsing can help you identify and fix issues in your code, as well as optimize performance.\n",
        "* **Parsing strange input**: Parsing is not limited to programming languages. It can be applied to any structured data format, such as JSON, XML, or even natural language processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-I6UBIyKuPq"
      },
      "source": [
        "## A simple arithmetic parser and interpreter\n",
        "\n",
        "First let's start with an informal way to create an interpreter for super simple arithmetic expressions. For now we will only support addition and subtraction of integers. We will not use any formal specification or grammar, but let's try to \"wing it\" and see how far we can get. We will use Python for this example, but the concepts apply to any programming language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OwgNBpcbKuPr",
        "outputId": "9fb74361-8d7b-4339-f8db-1a92bc916270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# we we will try to do what Python does when we do eval\n",
        "eval(\"5 + 6 + 7\")  # this is dangerous if you do not control the string !\n",
        "# generally eval is not used in production code due to safety issues\n",
        "# so eval will lex the string and parse it and then actually do the work (meaning summing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zt124NSbKuPs",
        "outputId": "8bde6e34-4571-431e-d931-c699934f117e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['5', '+', '6', '+', '7']\n"
          ]
        }
      ],
      "source": [
        "# so we will start with trying to do parser for expressions such as \"1 + 2 + 3\" just addition and numbers\n",
        "# to do so we will simply split the string by spaces and then we will check if the first element is a number or not\n",
        "text = \"5 + 6 + 7\"\n",
        "tokens = text.split(\" \") # note there is no real check for what makes a \"valid\" token here\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v5ufcVWPKuPs",
        "outputId": "4f69f2ed-1e25-47de-b665-34ba69df7655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens ['5', '+', '6', '+', '7']\n",
            "18\n",
            "Can anyone point out the problem with this code?\n"
          ]
        }
      ],
      "source": [
        "# how would we interpret this expression?\n",
        "# we simply process the tokens one by one\n",
        "# we will start with the first token and check if it is a number or not\n",
        "# if it is a number we will store it in a variable and then we will check the next token\n",
        "# if it is a number we will add it to the variable\n",
        "result = 0\n",
        "for token in tokens:\n",
        "    if token.isnumeric():\n",
        "        # if the token is a number we will store it in a variable\n",
        "        result += int(token)\n",
        "    else:\n",
        "        # if the token is not a number we will check if it is a plus sign\n",
        "        if token == \"+\":\n",
        "            # if it is a plus sign we will check the next token\n",
        "            # and add it to the result\n",
        "            continue\n",
        "        else:\n",
        "            # if it is not a plus sign we will raise an error\n",
        "            print(\"Invalid token\", token)\n",
        "            raise ValueError(\"Invalid token\")\n",
        "print(\"Tokens\", tokens)\n",
        "print(result)\n",
        "print(\"Can anyone point out the problem with this code?\")\n",
        "# Hint: it is too liberal in what it accepts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weird_input = \"50 30 75\"\n",
        "tokens = weird_input.split()\n",
        "for token in tokens:\n",
        "    if token.isnumeric():\n",
        "        # if the token is a number we will store it in a variable\n",
        "        result += int(token)\n",
        "    else:\n",
        "        # if the token is not a number we will check if it is a plus sign\n",
        "        if token == \"+\":\n",
        "            # if it is a plus sign we will check the next token\n",
        "            # and add it to the result\n",
        "            continue\n",
        "        else:\n",
        "            # if it is not a plus sign we will raise an error\n",
        "            print(\"Invalid token\", token)\n",
        "            raise ValueError(\"Invalid token\")\n",
        "print(\"Tokens\", tokens)\n",
        "print(result)\n",
        "print(\"Can anyone point out the problem with this code?\")"
      ],
      "metadata": {
        "id": "U8IQLrr7NjlV",
        "outputId": "512d5ff4-65f0-4460-fe3d-f6f812603124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens ['50', '30', '75']\n",
            "173\n",
            "Can anyone point out the problem with this code?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# well above could be handy if you tried to implement a sum over an iterator (list, tuple etc)\n",
        "# however it is not quite what we want"
      ],
      "metadata": {
        "id": "5KAg_McoN6Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaIawCy0KuPs"
      },
      "source": [
        "### Fixing the adder evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ERk0t6zAKuPt",
        "outputId": "c6062389-0a2b-42cf-e7da-b5ba6c1cd0eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1180\n"
          ]
        }
      ],
      "source": [
        "# let's create a function that will do the same thing\n",
        "# we want to fix the code so that it will not accept invalid tokens\n",
        "# this means that we do not take two numbers in a row\n",
        "def evaluate_add_expression(expression):\n",
        "    tokens = expression.split(\" \") # we have different ways of tokenization here we use single whitespace\n",
        "    result = 0 # could call this accumulator or acc for short\n",
        "    last_was_number = False\n",
        "    for token in tokens:\n",
        "        if token.isnumeric():\n",
        "            if last_was_number:\n",
        "                print(\"Invalid token two numbers in a row not allowed\", token)\n",
        "                raise ValueError(\"Invalid token two numbers in a row not allowed\")\n",
        "            result += int(token)\n",
        "            last_was_number = True\n",
        "        else:\n",
        "            if token == \"+\":\n",
        "                last_was_number = False\n",
        "                continue\n",
        "            else:\n",
        "                print(\"Invalid token\", token)\n",
        "                raise ValueError(\"Invalid token\", token)\n",
        "    return result\n",
        "# let's test the function\n",
        "print(evaluate_add_expression(\"50 + 60 + 70 + 1000\"))\n",
        "# try it with negative numbers, what will happen?\n",
        "# print(evaluate_add_expression(\"50 + 60 + 70 + -1000\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's try it with some bad tokens...\n",
        "print(evaluate_add_expression(\"50 + 60 + 70 + -1000\"))\n"
      ],
      "metadata": {
        "id": "3YNwLIvOOe4F",
        "outputId": "d6530f3d-a08d-4d7c-abdf-63d057c6f3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid token -1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "('Invalid token', '-1000')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f011bb46372a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now let's try it with some bad tokens...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_add_expression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"50 + 60 + 70 + -1000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7d5cfcf32fe1>\u001b[0m in \u001b[0;36mevaluate_add_expression\u001b[0;34m(expression)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# let's test the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ('Invalid token', '-1000')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how about a bunch of numbers\n",
        "print(evaluate_add_expression(\"50 30 75\"))"
      ],
      "metadata": {
        "id": "5-izMoLFOqay",
        "outputId": "093fe812-7f60-4355-a6b8-aa0f8fbdd0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid token two numbers in a row not allowed 30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid token two numbers in a row not allowed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-825041b21347>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# how about a bunch of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_add_expression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"50 30 75\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7d5cfcf32fe1>\u001b[0m in \u001b[0;36mevaluate_add_expression\u001b[0;34m(expression)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlast_was_number\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token two numbers in a row not allowed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token two numbers in a row not allowed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlast_was_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid token two numbers in a row not allowed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1TCWwkFKuPt"
      },
      "source": [
        "### Adding support for subtraction\n",
        "\n",
        "To improve our simple evaluator we will add support for substraction. This will involve a bit more of logic, but we will still keep it simple. We will use the same approach as before, but we will add a new function to handle substraction. We will also need to modify the `parse_expression` function to handle the new operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7qf0jAc1KuPt",
        "outputId": "8010d296-509b-4094-9475-cb42478e3e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing evaluate function\n",
            "110\n",
            "1180\n",
            "Tokens: ['50', '+', '60', '+', '70', '-', '100']\n",
            "80\n"
          ]
        }
      ],
      "source": [
        "# let's separate tokenization and evaluation into two functions\n",
        "def tokenize(expression):\n",
        "    return expression.split() # easy to implement and easy to modify\n",
        "def evaluate(tokens, verbose=False):\n",
        "    if verbose:\n",
        "        print(\"Tokens:\", tokens)\n",
        "    result = 0\n",
        "    token_types = {\"NUMBER\", \"PLUS\", \"MINUS\"} # Python does not have true enums, this is more for documentation\n",
        "    # we will use a dictionary to map the tokens to their types\n",
        "\n",
        "    valid_operators = {\"+\":\"PLUS\",\n",
        "                       \"-\":\"MINUS\"}\n",
        "\n",
        "    # we do not want to allow two NUMBERS nor two OPERATORS in a row\n",
        "    # so we will check the type of the token and if it is the same as the previous one we will raise an error\n",
        "    last_token = \"PLUS\" # to add the first number we add a ficticuous PLUS operator this way we will verify whether the first token is number\n",
        "    # we want to do everything in one pass - so called single pass parser\n",
        "    for token in tokens:\n",
        "        if token in valid_operators: # we check valid_operator keys\n",
        "            if last_token in valid_operators:\n",
        "                print(\"Invalid token two in row\", token)\n",
        "                raise ValueError(\"Invalid token, two operators in a row\")\n",
        "            last_token = valid_operators[token] # we use the dictionary to get symbolic name for operators\n",
        "            # we do nothing else for now\n",
        "            continue\n",
        "        # now let's try converting the token to a number\n",
        "        try:\n",
        "            number = int(token)\n",
        "            if last_token == \"NUMBER\":\n",
        "                print(\"Invalid token two numbers in row\", token)\n",
        "                raise ValueError(\"Invalid token, two numbers in a row\")\n",
        "\n",
        "            # now we actually perform the operation\n",
        "            if last_token == \"PLUS\":\n",
        "                result += number\n",
        "            elif last_token == \"MINUS\":\n",
        "                result -= number\n",
        "            # we need to update the last token to be a number\n",
        "            last_token = \"NUMBER\"\n",
        "        except ValueError:\n",
        "            print(\"Invalid token not a number NOR operator\", token)\n",
        "            raise ValueError(\"Invalid token not a number NOR operator\")\n",
        "            # note we could just ignore invalid tokens that is a fine approach sometimes\n",
        "\n",
        "    return result\n",
        "\n",
        "# let's test the function\n",
        "print(\"Testing evaluate function\")\n",
        "print(evaluate(tokenize(\"50 + 60\")))\n",
        "print(evaluate(tokenize(\"50 + 60 + 70 + 1000\")))\n",
        "# let's test the function with negative numbers\n",
        "print(evaluate(tokenize(\"50 + 60 + 70 - 100\"), verbose=True))\n",
        "# still not raising error on multiple numbers\n",
        "# print(evaluate(tokenize(\"50 + 60 100 200 300\"))) # works properly\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOptTEYbKuPt"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKAM_ywLKuPt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Components of a Parser**\n",
        "A traditional parser comprises two main stages:\n",
        "\n",
        "1. **Lexical Analysis (Lexer/Scanner)**: Converts the raw character stream into a sequence of tokens, each representing atomic elements like identifiers, keywords, literals, or operators .\n",
        "2. **Syntactic Analysis (Parser)**: Consumes the token stream to build a parse tree or abstract syntax tree (AST) according to production rules defined in the grammar .\n",
        "\n",
        "![Parse Tree vs AST](https://raw.githubusercontent.com/ValRCS/RTU_Algorithms_DIP321/refs/heads/main/imgs/parse_tree_vs_ast.png)\n",
        "\n",
        "In the AST the indication of the specific operator has disappeared and all that remains is the operation to be performed. The specific operator is an example of an intermediate rule. Src: https://tomassetti.me/guide-parsing-algorithms-terminology/\n",
        "\n",
        "Some modern tools use **scannerless parsers**, which merge tokenization and parsing into a single step, treating character sequences and syntactic constructs uniformly .\n",
        "\n",
        "---\n",
        "## Compiler versus Interpreter Parsing\n",
        "\n",
        "In a **compiler**, parsing is a critical phase that translates high-level source code into an intermediate representation (IR) or machine code. The parser checks for syntactic correctness and builds a parse tree or abstract syntax tree (AST) that reflects the program's structure.\n",
        "\n",
        "In an **interpreter**, parsing is often more dynamic, as it may involve interpreting code on-the-fly. The parser may need to handle incomplete or incremental input, such as in interactive environments or REPLs (Read-Eval-Print Loops). In these cases, the parser must be able to adapt to partial inputs and provide immediate feedback or execution results.\n",
        "\n",
        "## **Grammar Families**\n",
        "Grammars formalize the syntax of a language. The two primary classes are:\n",
        "\n",
        "* **Regular Grammars**: Describe *regular languages* that can be recognized by finite-state machines (and thus by regular expressions). They cannot express nested or recursive constructs .\n",
        "* **Context-Free Grammars (CFGs)**: More powerful; can describe recursive, nested structures common in programming languages. Recognized by pushdown automata and form the basis for most parser generators .\n",
        "\n",
        "A third practical formalism is **Parsing Expression Grammars (PEGs)**, which resemble CFGs but resolve ambiguities by ordered choice and naturally pair with *packrat* (memoizing) parsers .\n",
        "\n",
        "Grammars have a direct relation to Automata Theory:\n",
        "https://en.wikipedia.org/wiki/Automata_theory\n",
        "\n",
        "There are different Grammars that correspond to different Automatons\n",
        "\n",
        "---\n",
        "\n",
        "## **Parsing Strategies**\n",
        "Parsers are classified by the order in which they traverse the parse tree:\n",
        "\n",
        "* **Top-Down Parsers** build the tree from the root downward, expanding nonterminals as they match tokens. They include LL-family algorithms (e.g., LL(1)) and **recursive descent** parsers .\n",
        "* **Bottom-Up Parsers** build from the leaves upward, recognizing small constructs and reducing them to nonterminals until reaching the start symbol. The classic example is the LR-family (e.g., LR(1), LALR) .\n",
        "\n",
        "Chart parsers such as **Earley** and **CYK** blend strategies using dynamic programming to handle all CFGs (Earley) or CFGs in Chomsky Normal Form (CYK) .\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Parsing Algorithms**\n",
        "\n",
        "| **Algorithm**         | **Strategy & Grammar**                  | **Complexity**              | **Notes**                                                        |\n",
        "| --------------------- | --------------------------------------- | --------------------------- | ---------------------------------------------------------------- |\n",
        "| **LL(1)**             | Top-Down, CFG without left recursion    | O(n)                        | Table-driven; requires careful grammar structuring               |\n",
        "| **Recursive Descent** | Top-Down, backtracking or predictive    | O(n⁴) worst-case            | Easy to hand-write; tail-recursion schemes handle left-recursion |\n",
        "| **LR(1) / LALR**      | Bottom-Up, all deterministic CFG (LALR) | O(n)                        | Powerful; tables large—used via generators (e.g., Bison)         |\n",
        "| **Earley**            | Top-Down chart, all CFG                 | O(n³) worst, \\~O(n) average | No grammar restrictions; prediction + completion phases          |\n",
        "| **CYK**               | Bottom-Up chart, CNF-required CFG       | O(n³)                       | Theoretical importance; impractical for general parsing          |\n",
        "| **Packrat (PEG)**     | Top-Down with memoization, PEG          | O(n) average                | Unlimited lookahead; high memory usage; ordered choice           |\n",
        "\n",
        "Each algorithm offers trade-offs in terms of grammar expressiveness, performance guarantees, ease of implementation, and memory consumption .\n",
        "\n",
        "---\n",
        "\n",
        "## **Parse Trees vs. Abstract Syntax Trees**\n",
        "\n",
        "* **Parse Tree (Concrete Syntax Tree):** Mirrors the structure of the grammar exactly, including all intermediate symbols and sometimes punctuation tokens .\n",
        "* **Abstract Syntax Tree (AST):** A pruned, higher-level representation that retains only semantically significant constructs (e.g., operator nodes, control-flow constructs) and omits syntactic sugar like parentheses or literal tokens .\n",
        "\n",
        "Transforming a parse tree into an AST simplifies downstream compiler or analysis phases by focusing on the core meaning of the code.\n",
        "\n",
        "---\n",
        "\n",
        "## **Choosing an Algorithm**\n",
        "\n",
        "* For **hand-crafted** parsers or small DSLs, recursive descent or Pratt parsers yield fast development.\n",
        "* For **production** compilers, LALR or LR(1) via generator tools balance performance and grammar power.\n",
        "* For **extensible** systems (e.g., IDEs supporting many languages) or educational purposes, Earley offers simplicity at some performance cost.\n",
        "* For **PEG-style** grammars requiring unambiguous ordered choice, packrat parsing ensures linear performance when memory permits.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9FOeSpKuPu"
      },
      "source": [
        "## Tooling for Parsing\n",
        "\n",
        "Usually you do not want to implement a parser from scratch. Instead, you can use existing libraries or tools that provide parsing capabilities. Here are some popular options:\n",
        "\n",
        "### Parser Generators\n",
        "\n",
        "Parser generators automate the creation of parsers from formal grammar specifications. They typically generate code for a specific parsing algorithm, such as LALR or LL(1). Some popular parser generators include:\n",
        "* **ANTLR**: A powerful parser generator that supports multiple languages and generates parsers in Java, C#, Python, and more. It uses LL(*) parsing and can handle complex grammars. - https://www.antlr.org/\n",
        "* **Bison**: A widely used parser generator for C/C++ that implements LALR(1) parsing. It is often used in conjunction with Flex for lexical analysis. - https://www.gnu.org/software/bison/\n",
        "* **Yacc**: An older parser generator for C that also implements LALR(1) parsing. It is less commonly used today but still relevant in some legacy systems. - [https://dinosaur.compi    lertools.net/yacc/](https://en.wikipedia.org/wiki/Yacc)\n",
        "* **PEG.js**: A parser generator for JavaScript that uses Parsing Expression Grammars (PEGs). It is suitable for building parsers for DSLs and other applications. - https://github.com/pegjs/pegjs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClvDjFEoKuPu"
      },
      "source": [
        "### How Antler Works\n",
        "\n",
        "![Antler example](https://raw.githubusercontent.com/ValRCS/RTU_Algorithms_DIP321/refs/heads/main/imgs/antler_parser.jpg)\n",
        "\n",
        "Try it yourself: http://lab.antlr.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBg5AoFwKuPu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMCUvgcKuPu"
      },
      "source": [
        "## Full Arithmetic Parser\n",
        "\n",
        "To implement a full arithmetic parser we will separate tokenization and parsing. We will use a blank AST class to represent the tree.\n",
        "\n",
        "We use EBNF to define the grammar.\n",
        "\n",
        "Grammar (EBNF):\n",
        "```\n",
        "\n",
        "expression ::= term { (\"+\" | \"-\") term } ;\n",
        "term       ::= factor { (\"*\" | \"/\") factor } ;\n",
        "factor     ::= integer | \"(\" expression \")\" ;\n",
        "digit_excluding_zero ::= \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ;\n",
        "digit                ::= \"0\" | digit_excluding_zero ;\n",
        "positive_integer ::= digit_excluding_zero, { digit } ;\n",
        "integer ::= \"0\" | [ \"-\" ], positive_integer  ;\n",
        "\n",
        "```\n",
        "\n",
        "### EBNF parsers online\n",
        "\n",
        "You can test your EBNF grammar online with the following tools:\n",
        "\n",
        "* [EBNF Evaluator](https://mdkrajnak.github.io/ebnftest/)\n",
        "\n",
        "### BNF parser\n",
        "\n",
        "BNF is more limited than EBNF. It does not support optionality or repetition.\n",
        "BNF can define the same grammar as EBNF, but it is more verbose.\n",
        "\n",
        "\n",
        "\n",
        "* [BNF Playground](https://bnfplayground.pauliankline.com/)\n",
        "\n",
        "More discussion on EBNF can be found in the [Wikipedia article](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FV6vpgTgKuPu",
        "outputId": "7e060807-6470-403d-bbb5-1b10125034ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.0\n",
            "84\n",
            "42.0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "# Grammar (EBNF):\n",
        "# expression = term { (\"+\" | \"-\") term } ;\n",
        "# term       = factor { (\"*\" | \"/\") factor } ;\n",
        "# factor     = INTEGER | \"(\" expression \")\" ;\n",
        "# INTEGER    = [0-9]+ ; # here I sort of skip the full definition of INTEGER as above\n",
        "\n",
        "Token = namedtuple('Token', ['type', 'value'])\n",
        "\n",
        "TOKEN_SPEC = [\n",
        "    ('INTEGER', r'\\d+'),\n",
        "    ('PLUS',    r'\\+'),\n",
        "    ('MINUS',   r'-'),\n",
        "    ('MUL',     r'\\*'),\n",
        "    ('DIV',     r'/'),\n",
        "    ('LPAREN',  r'\\('),\n",
        "    ('RPAREN',  r'\\)'),\n",
        "    ('WS',      r'\\s+'),\n",
        "]\n",
        "\n",
        "master_pattern = re.compile(\n",
        "    '|'.join(f'(?P<{name}>{pattern})' for name, pattern in TOKEN_SPEC)\n",
        ")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Generate tokens from the input text.\"\"\"\n",
        "    for mo in master_pattern.finditer(text):\n",
        "        kind = mo.lastgroup\n",
        "        if kind == 'WS':\n",
        "            continue\n",
        "        value = mo.group()\n",
        "        yield Token(kind, value)\n",
        "    yield Token('EOF', '')\n",
        "\n",
        "# AST nodes\n",
        "typedef = None\n",
        "class AST:\n",
        "    pass\n",
        "\n",
        "class BinOp(AST):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op    # 'PLUS', 'MINUS', 'MUL', or 'DIV'\n",
        "        self.right = right\n",
        "\n",
        "class Num(AST):\n",
        "    def __init__(self, value):\n",
        "        self.value = int(value)\n",
        "\n",
        "# Parser with operator precedence\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = iter(tokens)\n",
        "        self.current_token = next(self.tokens)\n",
        "\n",
        "    def eat(self, token_type):\n",
        "        if self.current_token.type == token_type:\n",
        "            self.current_token = next(self.tokens)\n",
        "        else:\n",
        "            raise SyntaxError(f\"Expected {token_type}, got {self.current_token.type}\")\n",
        "\n",
        "    def parse(self):\n",
        "        node = self.parse_expression()\n",
        "        if self.current_token.type != 'EOF':\n",
        "            raise SyntaxError(\"Unexpected token after expression\")\n",
        "        return node\n",
        "\n",
        "    def parse_expression(self):\n",
        "        # expression = term { (+|-) term }\n",
        "        node = self.parse_term()\n",
        "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_term()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_term(self):\n",
        "        # term = factor { (*|/) factor }\n",
        "        node = self.parse_factor()\n",
        "        while self.current_token.type in ('MUL', 'DIV'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_factor()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_factor(self):\n",
        "        # factor = INTEGER | LPAREN expression RPAREN\n",
        "        token = self.current_token\n",
        "        if token.type == 'INTEGER':\n",
        "            self.eat('INTEGER')\n",
        "            return Num(token.value)\n",
        "        elif token.type == 'LPAREN':\n",
        "            self.eat('LPAREN')\n",
        "            node = self.parse_expression()\n",
        "            self.eat('RPAREN')\n",
        "            return node\n",
        "        else:\n",
        "            raise SyntaxError(f\"Unexpected token: {token.type}\")\n",
        "\n",
        "# Evaluator\n",
        "# this is a simple evaluator that assumes the AST is valid\n",
        "# we use recursion to evaluate the AST\n",
        "# we will use a simple recursive descent evaluator\n",
        "def evaluate(node):\n",
        "  if isinstance(node, Num):\n",
        "      return node.value\n",
        "  if isinstance(node, BinOp):\n",
        "      left = evaluate(node.left)\n",
        "      right = evaluate(node.right)\n",
        "      if node.op == 'PLUS':\n",
        "          return left + right\n",
        "      elif node.op == 'MINUS':\n",
        "          return left - right\n",
        "      elif node.op == 'MUL':\n",
        "          return left * right\n",
        "      elif node.op == 'DIV':\n",
        "          return left / right  # or integer division // if desired\n",
        "  raise ValueError(\"Unknown node type\")\n",
        "\n",
        "# Interpreter function\n",
        "def interpret(text):\n",
        "  tokens = tokenize(text)\n",
        "  parser = Parser(tokens)\n",
        "  ast = parser.parse()\n",
        "  return evaluate(ast)\n",
        "\n",
        "# Examples\n",
        "\n",
        "print(interpret(\"3 + 4 * (15 - 5) / 5\"))  # 3 + (4*(2-5)/5) = 3 + (4*(-3)/5) = 3 - 12/5 = 0.6\n",
        "print(interpret(\"(10 + 2) * 7\"))       # (10+2) * 7 = 84\n",
        "# how about multiple parentheses?\n",
        "print(interpret(\"((10 + 2) * 7) / (5 - 3)\")) # ((10+2)*7)/(5-3) = 84/2 = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful Link\n",
        "\n",
        "https://ruslanspivak.com/lsbasi-part7/"
      ],
      "metadata": {
        "id": "iCmUVM8JXIqs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhvu4odEKuPu"
      },
      "source": [
        "## Multiline support and variable assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gWDkrF4QKuPu",
        "outputId": "4895e918-1f6d-499c-eb0b-09759244a746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "# import re\n",
        "# from collections import namedtuple\n",
        "\n",
        "# Grammar (EBNF):\n",
        "# program    = statement { NEWLINE statement } ;\n",
        "# statement  = assignment | expression ;\n",
        "# assignment = IDENTIFIER \"=\" expression ;\n",
        "# expression = term { (\"+\" | \"-\") term } ;\n",
        "# term       = factor { (\"*\" | \"/\") factor } ;\n",
        "# factor     = INTEGER | IDENTIFIER | \"(\" expression \")\" ;\n",
        "\n",
        "Token = namedtuple('Token', ['type', 'value'])\n",
        "\n",
        "TOKEN_SPEC = [\n",
        "    ('INTEGER',    r'\\d+'),\n",
        "    ('IDENTIFIER', r'[A-Za-z_]\\w*'),\n",
        "    ('PLUS',       r'\\+'),\n",
        "    ('MINUS',      r'-'),\n",
        "    ('MUL',        r'\\*'),\n",
        "    ('DIV',        r'/'),\n",
        "    ('LPAREN',     r'\\('),\n",
        "    ('RPAREN',     r'\\)'),\n",
        "    ('EQ',         r'='),\n",
        "    ('WS',         r'\\s+'),\n",
        "]\n",
        "master_pattern = re.compile(\n",
        "    '|'.join(f'(?P<{name}>{pattern})' for name, pattern in TOKEN_SPEC)\n",
        ")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Generate tokens from the input text.\"\"\"\n",
        "    for mo in master_pattern.finditer(text):\n",
        "        kind = mo.lastgroup\n",
        "        if kind == 'WS':\n",
        "            continue\n",
        "        yield Token(kind, mo.group())\n",
        "    yield Token('EOF', '')\n",
        "\n",
        "# AST nodes\n",
        "class AST: pass\n",
        "\n",
        "class BinOp(AST):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op    # 'PLUS', 'MINUS', 'MUL', 'DIV'\n",
        "        self.right = right\n",
        "\n",
        "class Num(AST):\n",
        "    def __init__(self, value):\n",
        "        self.value = int(value)\n",
        "\n",
        "class Var(AST):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "# Parser with operator precedence and variables\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = list(tokens)\n",
        "        self.pos = 0\n",
        "        self.current_token = self.tokens[self.pos]\n",
        "\n",
        "    def eat(self, token_type):\n",
        "        if self.current_token.type == token_type:\n",
        "            self.pos += 1\n",
        "            self.current_token = self.tokens[self.pos]\n",
        "        else:\n",
        "            raise SyntaxError(f\"Expected {token_type}, got {self.current_token.type}\")\n",
        "\n",
        "    def parse(self):\n",
        "        return self.parse_expression()\n",
        "\n",
        "    def parse_expression(self):\n",
        "        node = self.parse_term()\n",
        "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_term()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_term(self):\n",
        "        node = self.parse_factor()\n",
        "        while self.current_token.type in ('MUL', 'DIV'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_factor()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_factor(self):\n",
        "        token = self.current_token\n",
        "        if token.type == 'INTEGER':\n",
        "            self.eat('INTEGER')\n",
        "            return Num(token.value)\n",
        "        if token.type == 'IDENTIFIER':\n",
        "            self.eat('IDENTIFIER')\n",
        "            return Var(token.value)\n",
        "        if token.type == 'LPAREN':\n",
        "            self.eat('LPAREN')\n",
        "            node = self.parse_expression()\n",
        "            self.eat('RPAREN')\n",
        "            return node\n",
        "        raise SyntaxError(f\"Unexpected token: {token.type}\")\n",
        "\n",
        "# Evaluator with variable environment\n",
        "\n",
        "def evaluate(node, env):\n",
        "    if isinstance(node, Num):\n",
        "        return node.value\n",
        "    if isinstance(node, Var):\n",
        "        if node.name in env:\n",
        "            return env[node.name]\n",
        "        raise NameError(f\"Undefined variable: {node.name}\")\n",
        "    if isinstance(node, BinOp):\n",
        "        left = evaluate(node.left, env)\n",
        "        right = evaluate(node.right, env)\n",
        "        if node.op == 'PLUS': return left + right\n",
        "        if node.op == 'MINUS': return left - right\n",
        "        if node.op == 'MUL': return left * right\n",
        "        if node.op == 'DIV': return left / right\n",
        "    raise ValueError(\"Unknown node type\")\n",
        "\n",
        "# Top-level interpreter with multiline and assignment support\n",
        "\n",
        "def interpret(program_text):\n",
        "    env = {}\n",
        "    last_value = None\n",
        "    for line in program_text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if '=' in line:\n",
        "            # assignment\n",
        "            name, expr = map(str.strip, line.split('=', 1))\n",
        "            if not re.match(r'^[A-Za-z_]\\w*$', name):\n",
        "                raise SyntaxError(f\"Invalid variable name: {name}\")\n",
        "            tokens = tokenize(expr)\n",
        "            parser = Parser(tokens)\n",
        "            ast = parser.parse()\n",
        "            value = evaluate(ast, env)\n",
        "            env[name] = value\n",
        "        else:\n",
        "            # expression\n",
        "            tokens = tokenize(line)\n",
        "            parser = Parser(tokens)\n",
        "            ast = parser.parse()\n",
        "            last_value = evaluate(ast, env)\n",
        "    return last_value\n",
        "\n",
        "\n",
        "prog = \"\"\"\n",
        "x = 5\n",
        "x * 10\n",
        "\"\"\"\n",
        "print(interpret(prog))  # 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gFLvQPCBKuPv",
        "outputId": "b29f52c8-c0b6-4b27-af83-aa4de2d5bfa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "prog = \"\"\"\n",
        "y = 10\n",
        "x = 5\n",
        "(x + y) * 2\n",
        "\"\"\"\n",
        "print(interpret(prog))  # 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prog = \"\"\"\n",
        "a = 10\n",
        "b = 20\n",
        "c = 30\n",
        "(a + b) * c + 1000 + 125\n",
        "\"\"\"\n",
        "print(interpret(prog))"
      ],
      "metadata": {
        "id": "guYeypzYYc8S",
        "outputId": "8bbd2425-1233-4227-e905-3361b7f653eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn3Nf9idKuPv"
      },
      "source": [
        "## Book Recommendation - Crafting Interpreters\n",
        "\n",
        "[Crafting Interpreters](https://craftinginterpreters.com/) by Robert Nystrom is an excellent resource for learning about parsing and building interpreters. The book covers both a tree-walk interpreter and a bytecode virtual machine, providing a comprehensive understanding of the concepts involved in interpreter design. It also includes practical examples and exercises to reinforce your learning.\n",
        "\n",
        "The author worked on the book for several years and it is a labor of love. The book is available for free online, and you can also purchase a physical copy if you prefer. The website includes a wealth of additional resources, including a blog, videos, and a community forum.\n",
        "\n",
        "The author cut his teeth working on games at Electronic Arts before moving on to Google, where he worked on the Dart programming language. He has a wealth of experience in programming languages and compiler design, and his passion for the subject shines through in the book.\n",
        "\n",
        "![Crafting Interpreters](https://craftinginterpreters.com/image/header.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}