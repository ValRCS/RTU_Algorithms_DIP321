{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdeZ_PO_witn"
      },
      "outputs": [],
      "source": [
        "# so we want to parse input/strings such as \"5 + 6 + 10\"\n",
        "# eventually we would want to move to \"5+6 * 10+3\" and parse that correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJTVB6Nwitr"
      },
      "outputs": [],
      "source": [
        "# for addition we do not need anything fance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuibxQzawnzX",
        "outputId": "dea8870e-c7dc-4003-b7ed-2004e6b0b1f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval(\"5 + 6 + 7\")  # this is dangerous if you do not control the string ! \n",
        "# so eval will lex the string and parse it and then actually do the work (meaning summing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8qwaJVHwits",
        "outputId": "7bb458dc-1f7c-4c89-8e02-081c50154fc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"5 + 6 + 7\"\n",
        "# no significant whitespace\n",
        "clean = text.replace(\" \",\"\") # part of lexical analysis cleaning whitespace\n",
        "tokens = clean.split(\"+\") # tokenization again part of lexical analysis\n",
        "result = sum([int(token) for token in tokens]) # here we skip the tree since all of the tokens are separated by +..\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T0DyvKSWwitt"
      },
      "outputs": [],
      "source": [
        "def addIntrepreter(text):\n",
        "    clean = text.replace(\" \",\"\")\n",
        "    tokens = clean.split(\"+\")\n",
        "    result = sum([int(token) for token in tokens]) # nice shortcut because we only have +\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnQF7XpFwitt",
        "outputId": "6bf1e385-d2e1-45b6-b861-cbe0ac89de34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10032"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "addIntrepreter(\"  5+5+10000+5   + 7 + 10  \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_derVowitu"
      },
      "outputs": [],
      "source": [
        "# how about substraction well then we will already need some sort of structure we could use a stack based structure to store operation\n",
        "# \"10 - 5 + 3 - 2 + 20\"  should be 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ncAwml_witu"
      },
      "outputs": [],
      "source": [
        "# we could start again by stripping whitespace and similar as it is not signifant here\n",
        "# optimization would be to skip cleaning and clean whitespace as we go\n",
        "# then we could save the tokens in some sort of data structure (here stacks would work nicely)\n",
        "# or we could interpret as we go (so sort of like REPL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VacehPdVwitv"
      },
      "outputs": [],
      "source": [
        "def sub_add(text):\n",
        "    acc = 0\n",
        "    n = 0\n",
        "    tok = \"\"\n",
        "    state = \"NUM\" # \"OP\"\n",
        "    operations = [\"+\",\"-\"]\n",
        "    op = \"\"\n",
        "    # we really need a state machine here for determining whether we have a number or addition or substraction\n",
        "    # so ONE PASS parsing, scannerless parsing since it is so trivial\n",
        "    # so there is a simple state machine hidden here\n",
        "    for t in text:\n",
        "        if t in [\" \",\"\\t\"]: # same as replace or cleaning our insignifcant\n",
        "            continue\n",
        "        if t.isdigit():\n",
        "#             print(f\"Digit is {t} and tok is {tok}\")\n",
        "            if state == \"OP\":\n",
        "                state = \"NUM\"\n",
        "                tok = \"\" # not efficient keep building up the NUM\n",
        "            tok += t\n",
        "#             print(f\"Digit is {t} and tok AFTER is {tok}\")\n",
        "            continue\n",
        "        if t in operations:\n",
        "            state = \"OP\" # FIXME multiple operations error\n",
        "            print(f\"BEFORE operation  {acc} {op} {tok}\")\n",
        "            if op == \"+\": # we check the previous operation\n",
        "                acc += int(tok)\n",
        "                tok = \"\"\n",
        "            elif op == \"-\":\n",
        "                acc -= int(tok)\n",
        "                tok = \"\"\n",
        "            elif op == \"\": # first time\n",
        "                acc = int(tok)\n",
        "                tok = \"\"\n",
        "            print(f\"AFTER operation {op} {acc}\")\n",
        "            op = t\n",
        "    if op == \"+\": # we check the previous operation\n",
        "        acc += int(tok)\n",
        "    elif op == \"-\":\n",
        "        acc -= int(tok)\n",
        "    return acc\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1LHRssbwitv",
        "outputId": "ba865981-ff24-44c6-d44e-9d473534e64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE operation  0  10\n",
            "AFTER operation  10\n",
            "BEFORE operation  10 - 5\n",
            "AFTER operation - 5\n",
            "BEFORE operation  5 + 3\n",
            "AFTER operation + 8\n",
            "BEFORE operation  8 - 2\n",
            "AFTER operation - 6\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_add(\"10 - 5 + 3 - 2 + 20\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz0pYGe9witx"
      },
      "outputs": [],
      "source": [
        "# for more complicated operations we will need to build a syntax tree we can't just have a simple accumulator design, \n",
        "# above is only sufficient when we have left to right order of operations\n",
        "\n",
        "# one example is given in this course\n",
        "# https://ruslanspivak.com/lsbasi-part1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Arithmetic Parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6000000000000001\n",
            "84\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "# Grammar (EBNF):\n",
        "# expression = term { (\"+\" | \"-\") term } ;\n",
        "# term       = factor { (\"*\" | \"/\") factor } ;\n",
        "# factor     = INTEGER | \"(\" expression \")\" ;\n",
        "# INTEGER    = [0-9]+ ;\n",
        "\n",
        "Token = namedtuple('Token', ['type', 'value'])\n",
        "\n",
        "TOKEN_SPEC = [\n",
        "    ('INTEGER', r'\\d+'),\n",
        "    ('PLUS',    r'\\+'),\n",
        "    ('MINUS',   r'-'),\n",
        "    ('MUL',     r'\\*'),\n",
        "    ('DIV',     r'/'),\n",
        "    ('LPAREN',  r'\\('),\n",
        "    ('RPAREN',  r'\\)'),\n",
        "    ('WS',      r'\\s+'),\n",
        "]\n",
        "\n",
        "master_pattern = re.compile(\n",
        "    '|'.join(f'(?P<{name}>{pattern})' for name, pattern in TOKEN_SPEC)\n",
        ")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Generate tokens from the input text.\"\"\"\n",
        "    for mo in master_pattern.finditer(text):\n",
        "        kind = mo.lastgroup\n",
        "        if kind == 'WS':\n",
        "            continue\n",
        "        value = mo.group()\n",
        "        yield Token(kind, value)\n",
        "    yield Token('EOF', '')\n",
        "\n",
        "# AST nodes\n",
        "typedef = None\n",
        "class AST:\n",
        "    pass\n",
        "\n",
        "class BinOp(AST):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op    # 'PLUS', 'MINUS', 'MUL', or 'DIV'\n",
        "        self.right = right\n",
        "\n",
        "class Num(AST):\n",
        "    def __init__(self, value):\n",
        "        self.value = int(value)\n",
        "\n",
        "# Parser with operator precedence\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = iter(tokens)\n",
        "        self.current_token = next(self.tokens)\n",
        "\n",
        "    def eat(self, token_type):\n",
        "        if self.current_token.type == token_type:\n",
        "            self.current_token = next(self.tokens)\n",
        "        else:\n",
        "            raise SyntaxError(f\"Expected {token_type}, got {self.current_token.type}\")\n",
        "\n",
        "    def parse(self):\n",
        "        node = self.parse_expression()\n",
        "        if self.current_token.type != 'EOF':\n",
        "            raise SyntaxError(\"Unexpected token after expression\")\n",
        "        return node\n",
        "\n",
        "    def parse_expression(self):\n",
        "        # expression = term { (+|-) term }\n",
        "        node = self.parse_term()\n",
        "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_term()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_term(self):\n",
        "        # term = factor { (*|/) factor }\n",
        "        node = self.parse_factor()\n",
        "        while self.current_token.type in ('MUL', 'DIV'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_factor()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_factor(self):\n",
        "        # factor = INTEGER | LPAREN expression RPAREN\n",
        "        token = self.current_token\n",
        "        if token.type == 'INTEGER':\n",
        "            self.eat('INTEGER')\n",
        "            return Num(token.value)\n",
        "        elif token.type == 'LPAREN':\n",
        "            self.eat('LPAREN')\n",
        "            node = self.parse_expression()\n",
        "            self.eat('RPAREN')\n",
        "            return node\n",
        "        else:\n",
        "            raise SyntaxError(f\"Unexpected token: {token.type}\")\n",
        "\n",
        "# Evaluator\n",
        "def evaluate(node):\n",
        "  if isinstance(node, Num):\n",
        "      return node.value\n",
        "  if isinstance(node, BinOp):\n",
        "      left = evaluate(node.left)\n",
        "      right = evaluate(node.right)\n",
        "      if node.op == 'PLUS':\n",
        "          return left + right\n",
        "      elif node.op == 'MINUS':\n",
        "          return left - right\n",
        "      elif node.op == 'MUL':\n",
        "          return left * right\n",
        "      elif node.op == 'DIV':\n",
        "          return left / right  # or integer division // if desired\n",
        "  raise ValueError(\"Unknown node type\")\n",
        "\n",
        "# Interpreter function\n",
        "def interpret(text):\n",
        "  tokens = tokenize(text)\n",
        "  parser = Parser(tokens)\n",
        "  ast = parser.parse()\n",
        "  return evaluate(ast)\n",
        "\n",
        "# Examples\n",
        "if __name__ == '__main__':\n",
        "  print(interpret(\"3 + 4 * (2 - 5) / 5\"))  # 3 + (4*(2-5)/5) = 3 + (4*(-3)/5) = 3 - 12/5 = 0.6\n",
        "  print(interpret(\"(10 + 2) * 7\"))       # (10+2) * 7 = 84\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
