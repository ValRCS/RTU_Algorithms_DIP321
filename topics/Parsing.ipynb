{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parsing Algorithms\n",
        "\n",
        "**Overview of Parsing**\n",
        "Parsing is the process of analyzing a raw input (typically source code) according to the rules of a formal grammar to uncover its underlying structure . In compilers and interpreters, parsing bridges the gap between human-readable code and machine-friendly representations by transforming text into structured trees for further semantic checks or code generation .\n",
        "\n",
        "## Why should you care about parsing?\n",
        "\n",
        "Parsing is a fundamental concept in computer science, especially in the context of programming languages and compilers. Understanding parsing helps you:\n",
        "* **Design languages**: Knowing how parsing works allows you to create languages such as Domain Specific Languages with clear and unambiguous syntax.\n",
        "* **Implement compilers**: Parsing is a crucial step in compiling code, and understanding it helps you build efficient and effective compilers.\n",
        "* **Debug and optimize**: Understanding parsing can help you identify and fix issues in your code, as well as optimize performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple arithmetic parser and interpreter\n",
        "\n",
        "First let's start with an informal way to create an interpreter for super simple arithmetic expressions. For now we will only support addition and subtraction of integers. We will not use any formal specification or grammar, but let's try to \"wing it\" and see how far we can get. We will use Python for this example, but the concepts apply to any programming language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1', '+', '2', '+', '3']\n"
          ]
        }
      ],
      "source": [
        "# so we will start with trying to do parser for expressions such as \"1 + 2 + 3\" just addition and numbers\n",
        "# to do so we will simply split the string by spaces and then we will check if the first element is a number or not\n",
        "text = \"1 + 2 + 3\"\n",
        "tokens = text.split(\" \")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "# how would we interpret this expression?\n",
        "# we simply process the tokens one by one\n",
        "# we will start with the first token and check if it is a number or not\n",
        "# if it is a number we will store it in a variable and then we will check the next token\n",
        "# if it is a number we will add it to the variable\n",
        "result = 0\n",
        "for token in tokens:\n",
        "    if token.isnumeric():\n",
        "        # if the token is a number we will store it in a variable\n",
        "        result += int(token)\n",
        "    else:\n",
        "        # if the token is not a number we will check if it is a plus sign\n",
        "        if token == \"+\":\n",
        "            # if it is a plus sign we will check the next token\n",
        "            # and add it to the result\n",
        "            continue\n",
        "        else:\n",
        "            # if it is not a plus sign we will raise an error\n",
        "            print(\"Invalid token\", token)\n",
        "            raise ValueError(\"Invalid token\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1180\n"
          ]
        }
      ],
      "source": [
        "# let's create a function that will do the same thing\n",
        "def evaluate_add_expression(expression):\n",
        "    tokens = expression.split(\" \")\n",
        "    result = 0\n",
        "    for token in tokens:\n",
        "        if token.isnumeric():\n",
        "            result += int(token)\n",
        "        else:\n",
        "            if token == \"+\":\n",
        "                continue\n",
        "            else:\n",
        "                print(\"Invalid token\", token)\n",
        "                raise ValueError(\"Invalid token\")\n",
        "    return result\n",
        "# let's test the function\n",
        "print(evaluate_add_expression(\"50 + 60 + 70 + 1000\"))\n",
        "# try it with negative numbers, what will happen?\n",
        "# print(evaluate_expression(\"50 + 60 + 70 + -1000\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding support for substraction\n",
        "\n",
        "To improve our simple evaluator we will add support for substraction. This will involve a bit more of logic, but we will still keep it simple. We will use the same approach as before, but we will add a new function to handle substraction. We will also need to modify the `parse_expression` function to handle the new operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Components of a Parser**\n",
        "A traditional parser comprises two main stages:\n",
        "\n",
        "1. **Lexical Analysis (Lexer/Scanner)**: Converts the raw character stream into a sequence of tokens, each representing atomic elements like identifiers, keywords, literals, or operators .\n",
        "2. **Syntactic Analysis (Parser)**: Consumes the token stream to build a parse tree or abstract syntax tree (AST) according to production rules defined in the grammar .\n",
        "\n",
        "Some modern tools use **scannerless parsers**, which merge tokenization and parsing into a single step, treating character sequences and syntactic constructs uniformly .\n",
        "\n",
        "---\n",
        "## Compiler versus Interpreter Parsing\n",
        "\n",
        "In a **compiler**, parsing is a critical phase that translates high-level source code into an intermediate representation (IR) or machine code. The parser checks for syntactic correctness and builds a parse tree or abstract syntax tree (AST) that reflects the program's structure.\n",
        "\n",
        "In an **interpreter**, parsing is often more dynamic, as it may involve interpreting code on-the-fly. The parser may need to handle incomplete or incremental input, such as in interactive environments or REPLs (Read-Eval-Print Loops). In these cases, the parser must be able to adapt to partial inputs and provide immediate feedback or execution results.\n",
        "\n",
        "## **Grammar Families**\n",
        "Grammars formalize the syntax of a language. The two primary classes are:\n",
        "\n",
        "* **Regular Grammars**: Describe *regular languages* that can be recognized by finite-state machines (and thus by regular expressions). They cannot express nested or recursive constructs .\n",
        "* **Context-Free Grammars (CFGs)**: More powerful; can describe recursive, nested structures common in programming languages. Recognized by pushdown automata and form the basis for most parser generators .\n",
        "\n",
        "A third practical formalism is **Parsing Expression Grammars (PEGs)**, which resemble CFGs but resolve ambiguities by ordered choice and naturally pair with *packrat* (memoizing) parsers .\n",
        "\n",
        "---\n",
        "\n",
        "## **Parsing Strategies**\n",
        "Parsers are classified by the order in which they traverse the parse tree:\n",
        "\n",
        "* **Top-Down Parsers** build the tree from the root downward, expanding nonterminals as they match tokens. They include LL-family algorithms (e.g., LL(1)) and **recursive descent** parsers .\n",
        "* **Bottom-Up Parsers** build from the leaves upward, recognizing small constructs and reducing them to nonterminals until reaching the start symbol. The classic example is the LR-family (e.g., LR(1), LALR) .\n",
        "\n",
        "Chart parsers such as **Earley** and **CYK** blend strategies using dynamic programming to handle all CFGs (Earley) or CFGs in Chomsky Normal Form (CYK) .\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Parsing Algorithms**\n",
        "\n",
        "| **Algorithm**         | **Strategy & Grammar**                  | **Complexity**              | **Notes**                                                        |\n",
        "| --------------------- | --------------------------------------- | --------------------------- | ---------------------------------------------------------------- |\n",
        "| **LL(1)**             | Top-Down, CFG without left recursion    | O(n)                        | Table-driven; requires careful grammar structuring               |\n",
        "| **Recursive Descent** | Top-Down, backtracking or predictive    | O(n⁴) worst-case            | Easy to hand-write; tail-recursion schemes handle left-recursion |\n",
        "| **LR(1) / LALR**      | Bottom-Up, all deterministic CFG (LALR) | O(n)                        | Powerful; tables large—used via generators (e.g., Bison)         |\n",
        "| **Earley**            | Top-Down chart, all CFG                 | O(n³) worst, \\~O(n) average | No grammar restrictions; prediction + completion phases          |\n",
        "| **CYK**               | Bottom-Up chart, CNF-required CFG       | O(n³)                       | Theoretical importance; impractical for general parsing          |\n",
        "| **Packrat (PEG)**     | Top-Down with memoization, PEG          | O(n) average                | Unlimited lookahead; high memory usage; ordered choice           |\n",
        "\n",
        "Each algorithm offers trade-offs in terms of grammar expressiveness, performance guarantees, ease of implementation, and memory consumption .\n",
        "\n",
        "---\n",
        "\n",
        "## **Parse Trees vs. Abstract Syntax Trees**\n",
        "\n",
        "* **Parse Tree (Concrete Syntax Tree):** Mirrors the structure of the grammar exactly, including all intermediate symbols and sometimes punctuation tokens .\n",
        "* **Abstract Syntax Tree (AST):** A pruned, higher-level representation that retains only semantically significant constructs (e.g., operator nodes, control-flow constructs) and omits syntactic sugar like parentheses or literal tokens .\n",
        "\n",
        "Transforming a parse tree into an AST simplifies downstream compiler or analysis phases by focusing on the core meaning of the code.\n",
        "\n",
        "---\n",
        "\n",
        "## **Choosing an Algorithm**\n",
        "\n",
        "* For **hand-crafted** parsers or small DSLs, recursive descent or Pratt parsers yield fast development.\n",
        "* For **production** compilers, LALR or LR(1) via generator tools balance performance and grammar power.\n",
        "* For **extensible** systems (e.g., IDEs supporting many languages) or educational purposes, Earley offers simplicity at some performance cost.\n",
        "* For **PEG-style** grammars requiring unambiguous ordered choice, packrat parsing ensures linear performance when memory permits.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tooling for Parsing\n",
        "\n",
        "Usually you do not want to implement a parser from scratch. Instead, you can use existing libraries or tools that provide parsing capabilities. Here are some popular options:\n",
        "\n",
        "### Parser Generators\n",
        "\n",
        "Parser generators automate the creation of parsers from formal grammar specifications. They typically generate code for a specific parsing algorithm, such as LALR or LL(1). Some popular parser generators include:\n",
        "* **ANTLR**: A powerful parser generator that supports multiple languages and generates parsers in Java, C#, Python, and more. It uses LL(*) parsing and can handle complex grammars. - https://www.antlr.org/\n",
        "* **Bison**: A widely used parser generator for C/C++ that implements LALR(1) parsing. It is often used in conjunction with Flex for lexical analysis. - https://www.gnu.org/software/bison/\n",
        "* **Yacc**: An older parser generator for C that also implements LALR(1) parsing. It is less commonly used today but still relevant in some legacy systems. - [https://dinosaur.compi    lertools.net/yacc/](https://en.wikipedia.org/wiki/Yacc)\n",
        "* **PEG.js**: A parser generator for JavaScript that uses Parsing Expression Grammars (PEGs). It is suitable for building parsers for DSLs and other applications. - https://github.com/pegjs/pegjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdeZ_PO_witn"
      },
      "outputs": [],
      "source": [
        "# so we want to parse input/strings such as \"5 + 6 + 10\"\n",
        "# eventually we would want to move to \"5+6 * 10+3\" and parse that correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJTVB6Nwitr"
      },
      "outputs": [],
      "source": [
        "# for addition we do not need anything fance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuibxQzawnzX",
        "outputId": "dea8870e-c7dc-4003-b7ed-2004e6b0b1f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval(\"5 + 6 + 7\")  # this is dangerous if you do not control the string ! \n",
        "# so eval will lex the string and parse it and then actually do the work (meaning summing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8qwaJVHwits",
        "outputId": "7bb458dc-1f7c-4c89-8e02-081c50154fc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"5 + 6 + 7\"\n",
        "# no significant whitespace\n",
        "clean = text.replace(\" \",\"\") # part of lexical analysis cleaning whitespace\n",
        "tokens = clean.split(\"+\") # tokenization again part of lexical analysis\n",
        "result = sum([int(token) for token in tokens]) # here we skip the tree since all of the tokens are separated by +..\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T0DyvKSWwitt"
      },
      "outputs": [],
      "source": [
        "def addIntrepreter(text):\n",
        "    clean = text.replace(\" \",\"\")\n",
        "    tokens = clean.split(\"+\")\n",
        "    result = sum([int(token) for token in tokens]) # nice shortcut because we only have +\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnQF7XpFwitt",
        "outputId": "6bf1e385-d2e1-45b6-b861-cbe0ac89de34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10032"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "addIntrepreter(\"  5+5+10000+5   + 7 + 10  \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_derVowitu"
      },
      "outputs": [],
      "source": [
        "# how about substraction well then we will already need some sort of structure we could use a stack based structure to store operation\n",
        "# \"10 - 5 + 3 - 2 + 20\"  should be 26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ncAwml_witu"
      },
      "outputs": [],
      "source": [
        "# we could start again by stripping whitespace and similar as it is not signifant here\n",
        "# optimization would be to skip cleaning and clean whitespace as we go\n",
        "# then we could save the tokens in some sort of data structure (here stacks would work nicely)\n",
        "# or we could interpret as we go (so sort of like REPL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VacehPdVwitv"
      },
      "outputs": [],
      "source": [
        "def sub_add(text):\n",
        "    acc = 0\n",
        "    n = 0\n",
        "    tok = \"\"\n",
        "    state = \"NUM\" # \"OP\"\n",
        "    operations = [\"+\",\"-\"]\n",
        "    op = \"\"\n",
        "    # we really need a state machine here for determining whether we have a number or addition or substraction\n",
        "    # so ONE PASS parsing, scannerless parsing since it is so trivial\n",
        "    # so there is a simple state machine hidden here\n",
        "    for t in text:\n",
        "        if t in [\" \",\"\\t\"]: # same as replace or cleaning our insignifcant\n",
        "            continue\n",
        "        if t.isdigit():\n",
        "#             print(f\"Digit is {t} and tok is {tok}\")\n",
        "            if state == \"OP\":\n",
        "                state = \"NUM\"\n",
        "                tok = \"\" # not efficient keep building up the NUM\n",
        "            tok += t\n",
        "#             print(f\"Digit is {t} and tok AFTER is {tok}\")\n",
        "            continue\n",
        "        if t in operations:\n",
        "            state = \"OP\" # FIXME multiple operations error\n",
        "            print(f\"BEFORE operation  {acc} {op} {tok}\")\n",
        "            if op == \"+\": # we check the previous operation\n",
        "                acc += int(tok)\n",
        "                tok = \"\"\n",
        "            elif op == \"-\":\n",
        "                acc -= int(tok)\n",
        "                tok = \"\"\n",
        "            elif op == \"\": # first time\n",
        "                acc = int(tok)\n",
        "                tok = \"\"\n",
        "            print(f\"AFTER operation {op} {acc}\")\n",
        "            op = t\n",
        "    if op == \"+\": # we check the previous operation\n",
        "        acc += int(tok)\n",
        "    elif op == \"-\":\n",
        "        acc -= int(tok)\n",
        "    return acc\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1LHRssbwitv",
        "outputId": "ba865981-ff24-44c6-d44e-9d473534e64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE operation  0  10\n",
            "AFTER operation  10\n",
            "BEFORE operation  10 - 5\n",
            "AFTER operation - 5\n",
            "BEFORE operation  5 + 3\n",
            "AFTER operation + 8\n",
            "BEFORE operation  8 - 2\n",
            "AFTER operation - 6\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_add(\"10 - 5 + 3 - 2 + 20\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz0pYGe9witx"
      },
      "outputs": [],
      "source": [
        "# for more complicated operations we will need to build a syntax tree we can't just have a simple accumulator design, \n",
        "# above is only sufficient when we have left to right order of operations\n",
        "\n",
        "# one example is given in this course\n",
        "# https://ruslanspivak.com/lsbasi-part1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Arithmetic Parser\n",
        "\n",
        "To implement a full arithmetic parser we will separate tokenization and parsing. We will use a blank AST class to represent the tree. \n",
        "\n",
        "We use EBNF to define the grammar. \n",
        "\n",
        "Grammar (EBNF):\n",
        "```\n",
        "\n",
        "expression ::= term { (\"+\" | \"-\") term } ;\n",
        "term       ::= factor { (\"*\" | \"/\") factor } ;\n",
        "factor     ::= integer | \"(\" expression \")\" ;\n",
        "digit_excluding_zero ::= \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ;\n",
        "digit                ::= \"0\" | digit_excluding_zero ;\n",
        "positive_integer ::= digit_excluding_zero, { digit } ;\n",
        "integer ::= \"0\" | [ \"-\" ], positive_integer  ;\n",
        "\n",
        "```\n",
        "\n",
        "### EBNF parsers online\n",
        "\n",
        "You can test your EBNF grammar online with the following tools:\n",
        "\n",
        "* [EBNF Evaluator](https://mdkrajnak.github.io/ebnftest/)\n",
        "\n",
        "### BNF parser\n",
        "\n",
        "BNF is more limited than EBNF. It does not support optionality or repetition. \n",
        "BNF can define the same grammar as EBNF, but it is more verbose. \n",
        "\n",
        "\n",
        "\n",
        "* [BNF Playground](https://bnfplayground.pauliankline.com/)\n",
        "\n",
        "More discussion on EBNF can be found in the [Wikipedia article](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6000000000000001\n",
            "84\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import namedtuple\n",
        "\n",
        "# Grammar (EBNF):\n",
        "# expression = term { (\"+\" | \"-\") term } ;\n",
        "# term       = factor { (\"*\" | \"/\") factor } ;\n",
        "# factor     = INTEGER | \"(\" expression \")\" ;\n",
        "# INTEGER    = [0-9]+ ;\n",
        "\n",
        "Token = namedtuple('Token', ['type', 'value'])\n",
        "\n",
        "TOKEN_SPEC = [\n",
        "    ('INTEGER', r'\\d+'),\n",
        "    ('PLUS',    r'\\+'),\n",
        "    ('MINUS',   r'-'),\n",
        "    ('MUL',     r'\\*'),\n",
        "    ('DIV',     r'/'),\n",
        "    ('LPAREN',  r'\\('),\n",
        "    ('RPAREN',  r'\\)'),\n",
        "    ('WS',      r'\\s+'),\n",
        "]\n",
        "\n",
        "master_pattern = re.compile(\n",
        "    '|'.join(f'(?P<{name}>{pattern})' for name, pattern in TOKEN_SPEC)\n",
        ")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Generate tokens from the input text.\"\"\"\n",
        "    for mo in master_pattern.finditer(text):\n",
        "        kind = mo.lastgroup\n",
        "        if kind == 'WS':\n",
        "            continue\n",
        "        value = mo.group()\n",
        "        yield Token(kind, value)\n",
        "    yield Token('EOF', '')\n",
        "\n",
        "# AST nodes\n",
        "typedef = None\n",
        "class AST:\n",
        "    pass\n",
        "\n",
        "class BinOp(AST):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op    # 'PLUS', 'MINUS', 'MUL', or 'DIV'\n",
        "        self.right = right\n",
        "\n",
        "class Num(AST):\n",
        "    def __init__(self, value):\n",
        "        self.value = int(value)\n",
        "\n",
        "# Parser with operator precedence\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = iter(tokens)\n",
        "        self.current_token = next(self.tokens)\n",
        "\n",
        "    def eat(self, token_type):\n",
        "        if self.current_token.type == token_type:\n",
        "            self.current_token = next(self.tokens)\n",
        "        else:\n",
        "            raise SyntaxError(f\"Expected {token_type}, got {self.current_token.type}\")\n",
        "\n",
        "    def parse(self):\n",
        "        node = self.parse_expression()\n",
        "        if self.current_token.type != 'EOF':\n",
        "            raise SyntaxError(\"Unexpected token after expression\")\n",
        "        return node\n",
        "\n",
        "    def parse_expression(self):\n",
        "        # expression = term { (+|-) term }\n",
        "        node = self.parse_term()\n",
        "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_term()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_term(self):\n",
        "        # term = factor { (*|/) factor }\n",
        "        node = self.parse_factor()\n",
        "        while self.current_token.type in ('MUL', 'DIV'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_factor()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_factor(self):\n",
        "        # factor = INTEGER | LPAREN expression RPAREN\n",
        "        token = self.current_token\n",
        "        if token.type == 'INTEGER':\n",
        "            self.eat('INTEGER')\n",
        "            return Num(token.value)\n",
        "        elif token.type == 'LPAREN':\n",
        "            self.eat('LPAREN')\n",
        "            node = self.parse_expression()\n",
        "            self.eat('RPAREN')\n",
        "            return node\n",
        "        else:\n",
        "            raise SyntaxError(f\"Unexpected token: {token.type}\")\n",
        "\n",
        "# Evaluator\n",
        "def evaluate(node):\n",
        "  if isinstance(node, Num):\n",
        "      return node.value\n",
        "  if isinstance(node, BinOp):\n",
        "      left = evaluate(node.left)\n",
        "      right = evaluate(node.right)\n",
        "      if node.op == 'PLUS':\n",
        "          return left + right\n",
        "      elif node.op == 'MINUS':\n",
        "          return left - right\n",
        "      elif node.op == 'MUL':\n",
        "          return left * right\n",
        "      elif node.op == 'DIV':\n",
        "          return left / right  # or integer division // if desired\n",
        "  raise ValueError(\"Unknown node type\")\n",
        "\n",
        "# Interpreter function\n",
        "def interpret(text):\n",
        "  tokens = tokenize(text)\n",
        "  parser = Parser(tokens)\n",
        "  ast = parser.parse()\n",
        "  return evaluate(ast)\n",
        "\n",
        "# Examples\n",
        "\n",
        "print(interpret(\"3 + 4 * (2 - 5) / 5\"))  # 3 + (4*(2-5)/5) = 3 + (4*(-3)/5) = 3 - 12/5 = 0.6\n",
        "print(interpret(\"(10 + 2) * 7\"))       # (10+2) * 7 = 84\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiline support and variable assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "# import re\n",
        "# from collections import namedtuple\n",
        "\n",
        "# Grammar (EBNF):\n",
        "# program    = statement { NEWLINE statement } ;\n",
        "# statement  = assignment | expression ;\n",
        "# assignment = IDENTIFIER \"=\" expression ;\n",
        "# expression = term { (\"+\" | \"-\") term } ;\n",
        "# term       = factor { (\"*\" | \"/\") factor } ;\n",
        "# factor     = INTEGER | IDENTIFIER | \"(\" expression \")\" ;\n",
        "\n",
        "Token = namedtuple('Token', ['type', 'value'])\n",
        "\n",
        "TOKEN_SPEC = [\n",
        "    ('INTEGER',    r'\\d+'),\n",
        "    ('IDENTIFIER', r'[A-Za-z_]\\w*'),\n",
        "    ('PLUS',       r'\\+'),\n",
        "    ('MINUS',      r'-'),\n",
        "    ('MUL',        r'\\*'),\n",
        "    ('DIV',        r'/'),\n",
        "    ('LPAREN',     r'\\('),\n",
        "    ('RPAREN',     r'\\)'),\n",
        "    ('EQ',         r'='),\n",
        "    ('WS',         r'\\s+'),\n",
        "]\n",
        "master_pattern = re.compile(\n",
        "    '|'.join(f'(?P<{name}>{pattern})' for name, pattern in TOKEN_SPEC)\n",
        ")\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Generate tokens from the input text.\"\"\"\n",
        "    for mo in master_pattern.finditer(text):\n",
        "        kind = mo.lastgroup\n",
        "        if kind == 'WS':\n",
        "            continue\n",
        "        yield Token(kind, mo.group())\n",
        "    yield Token('EOF', '')\n",
        "\n",
        "# AST nodes\n",
        "class AST: pass\n",
        "\n",
        "class BinOp(AST):\n",
        "    def __init__(self, left, op, right):\n",
        "        self.left = left\n",
        "        self.op = op    # 'PLUS', 'MINUS', 'MUL', 'DIV'\n",
        "        self.right = right\n",
        "\n",
        "class Num(AST):\n",
        "    def __init__(self, value):\n",
        "        self.value = int(value)\n",
        "\n",
        "class Var(AST):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "# Parser with operator precedence and variables\n",
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = list(tokens)\n",
        "        self.pos = 0\n",
        "        self.current_token = self.tokens[self.pos]\n",
        "\n",
        "    def eat(self, token_type):\n",
        "        if self.current_token.type == token_type:\n",
        "            self.pos += 1\n",
        "            self.current_token = self.tokens[self.pos]\n",
        "        else:\n",
        "            raise SyntaxError(f\"Expected {token_type}, got {self.current_token.type}\")\n",
        "\n",
        "    def parse(self):\n",
        "        return self.parse_expression()\n",
        "\n",
        "    def parse_expression(self):\n",
        "        node = self.parse_term()\n",
        "        while self.current_token.type in ('PLUS', 'MINUS'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_term()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_term(self):\n",
        "        node = self.parse_factor()\n",
        "        while self.current_token.type in ('MUL', 'DIV'):\n",
        "            op = self.current_token.type\n",
        "            self.eat(op)\n",
        "            right = self.parse_factor()\n",
        "            node = BinOp(node, op, right)\n",
        "        return node\n",
        "\n",
        "    def parse_factor(self):\n",
        "        token = self.current_token\n",
        "        if token.type == 'INTEGER':\n",
        "            self.eat('INTEGER')\n",
        "            return Num(token.value)\n",
        "        if token.type == 'IDENTIFIER':\n",
        "            self.eat('IDENTIFIER')\n",
        "            return Var(token.value)\n",
        "        if token.type == 'LPAREN':\n",
        "            self.eat('LPAREN')\n",
        "            node = self.parse_expression()\n",
        "            self.eat('RPAREN')\n",
        "            return node\n",
        "        raise SyntaxError(f\"Unexpected token: {token.type}\")\n",
        "\n",
        "# Evaluator with variable environment\n",
        "\n",
        "def evaluate(node, env):\n",
        "    if isinstance(node, Num):\n",
        "        return node.value\n",
        "    if isinstance(node, Var):\n",
        "        if node.name in env:\n",
        "            return env[node.name]\n",
        "        raise NameError(f\"Undefined variable: {node.name}\")\n",
        "    if isinstance(node, BinOp):\n",
        "        left = evaluate(node.left, env)\n",
        "        right = evaluate(node.right, env)\n",
        "        if node.op == 'PLUS': return left + right\n",
        "        if node.op == 'MINUS': return left - right\n",
        "        if node.op == 'MUL': return left * right\n",
        "        if node.op == 'DIV': return left / right\n",
        "    raise ValueError(\"Unknown node type\")\n",
        "\n",
        "# Top-level interpreter with multiline and assignment support\n",
        "\n",
        "def interpret(program_text):\n",
        "    env = {}\n",
        "    last_value = None\n",
        "    for line in program_text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if '=' in line:\n",
        "            # assignment\n",
        "            name, expr = map(str.strip, line.split('=', 1))\n",
        "            if not re.match(r'^[A-Za-z_]\\w*$', name):\n",
        "                raise SyntaxError(f\"Invalid variable name: {name}\")\n",
        "            tokens = tokenize(expr)\n",
        "            parser = Parser(tokens)\n",
        "            ast = parser.parse()\n",
        "            value = evaluate(ast, env)\n",
        "            env[name] = value\n",
        "        else:\n",
        "            # expression\n",
        "            tokens = tokenize(line)\n",
        "            parser = Parser(tokens)\n",
        "            ast = parser.parse()\n",
        "            last_value = evaluate(ast, env)\n",
        "    return last_value\n",
        "\n",
        "\n",
        "prog = \"\"\"\n",
        "x = 5\n",
        "x * 10\n",
        "\"\"\"\n",
        "print(interpret(prog))  # 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "prog = \"\"\"\n",
        "y = 10\n",
        "x = 5\n",
        "(x + y) * 2\n",
        "\"\"\"\n",
        "print(interpret(prog))  # 30"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
